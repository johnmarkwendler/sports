{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnmarkwendler/sports/blob/main/waterpolo_ai_how_to_detect_track_and_identify_waterpolo_players.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS25QDv1a8_W"
      },
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# Water Polo AI: How to Detect, Track, and Identify Water Polo Players\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O5hUmxdbp0e"
      },
      "source": [
        "## Environment setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F8KpVPsbxz1"
      },
      "source": [
        "### Configure your API keys\n",
        "\n",
        "To run this notebook, you need to provide your HuggingFace Token and Roboflow API key.  \n",
        "\n",
        "- The `ROBOFLOW_API_KEY` is required to pull the fine-tuned RF-DETR player detector and the SmolVLM2 number recognizer from Roboflow Universe.  \n",
        "- The `HF_TOKEN` is required to pull the pretrained SigLIP model from HuggingFace.  \n",
        "\n",
        "Follow these steps:  \n",
        "\n",
        "- Open your [`HuggingFace Settings`](https://huggingface.co/settings) page. Click `Access Tokens` then `New Token` to generate a new token.  \n",
        "- Go to your [`Roboflow Settings`](https://app.roboflow.com/settings/api) page. Click `Copy`. This will place your private key in the clipboard.  \n",
        "- In Colab, go to the left pane and click on `Secrets` (ðŸ”‘).  \n",
        "    - Store the HuggingFace Access Token under the name `HF_TOKEN`.  \n",
        "    - Store the Roboflow API Key under the name `ROBOFLOW_API_KEY`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw1ZVBKxbvYM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "os.environ[\"ROBOFLOW_API_KEY\"] = userdata.get(\"ROBOFLOW_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfA2MikrcFWL"
      },
      "source": [
        "### Check GPU availability\n",
        "\n",
        "Let's make sure we have access to a GPU. Run the `nvidia-smi` command to verify. If you run into issues, go to `Runtime` -> `Change runtime type`, select `T4 GPU` or `L4 GPU`, and then click `Save`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKyA4lpocCLc"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkeEI66XcVgr"
      },
      "source": [
        "**NOTE:** To make it easier for us to manage datasets, images and models we create a `HOME` constant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpW8sBtBcHfG"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "HOME = Path.cwd()\n",
        "print(\"HOME:\", HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJT-DpbFt6Ns"
      },
      "source": [
        "## Install SAM2 real-time\n",
        "\n",
        "We will use `segment-anything-2-real-time`, an open-source fork of Metaâ€™s Segment Anything Model 2 optimized for real-time inference. After installing the repository, we will also download the required checkpoint files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfPi_Fi2uAu2"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Gy920/segment-anything-2-real-time.git\n",
        "%cd {HOME}/segment-anything-2-real-time\n",
        "!pip install -e . -q\n",
        "!python setup.py build_ext --inplace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8Uwlni-uzkf"
      },
      "outputs": [],
      "source": [
        "!(cd checkpoints && bash download_ckpts.sh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiTmIfgBcc1e"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j20VpRlOcXa2"
      },
      "outputs": [],
      "source": [
        "!pip install -q gdown\n",
        "!pip install -q inference-gpu\n",
        "\n",
        "!pip install supervision==0.27.0\n",
        "!pip install -q --upgrade git+https://github.com/johnmarkwendler/sports.git@main\n",
        "#!pip install -q git+https://github.com/roboflow/sports.git@feat/basketball\n",
        "\n",
        "!pip install -q transformers num2words\n",
        "# !pip install -q flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6lzxw4HfRxJ"
      },
      "source": [
        "Set the ONNX Runtime execution provider to CUDA to ensure model inference runs on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rU19OlpfPsG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"ONNXRUNTIME_EXECUTION_PROVIDERS\"] = \"[CUDAExecutionProvider]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWkLGOhnVpBG"
      },
      "source": [
        "### Source videos\n",
        "\n",
        "As an example, we will use sample videos from Game 1 of the 2025 NBA Playoffs between the Boston Celtics and the New York Knicks. We prepared 10 sample videos from this game. **We replaced this with Water Polo.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALpFXOu4cwL3"
      },
      "outputs": [],
      "source": [
        "SOURCE_VIDEO_DIRECTORY = HOME / \"source\"\n",
        "\n",
        "!gdown -q https://drive.google.com/drive/folders/1LivnYil1Yeg4GG71zCLUxKLhaOlqq_-X -O {SOURCE_VIDEO_DIRECTORY} --folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50Uq-moPBUBY"
      },
      "outputs": [],
      "source": [
        "!ls -la {SOURCE_VIDEO_DIRECTORY}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5LLKx8dE2se"
      },
      "outputs": [],
      "source": [
        "SOURCE_VIDEO_PATH = SOURCE_VIDEO_DIRECTORY / \"1-2short.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsHR16EVFBoo"
      },
      "source": [
        "### Team rosters\n",
        "\n",
        "We are preparing player rosters for both teams. We load the official lists that link jersey numbers to player names. These mappings will let us replace detected numbers with real names, making the final analytics clear and readable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZg88_zS7fIi"
      },
      "outputs": [],
      "source": [
        "TEAM_ROSTERS = {\n",
        "  \"USA\": {\n",
        "    \"1\": \"Weinberg\",\n",
        "    \"2\": \"Hooper\",\n",
        "    \"3\": \"Vavic\",\n",
        "    \"4\": \"Obert\",\n",
        "    \"5\": \"Daube\",\n",
        "    \"6\": \"Cupido\",\n",
        "    \"7\": \"Hallock\",\n",
        "    \"8\": \"Woodhead\",\n",
        "    \"9\": \"Bowen\",\n",
        "    \"10\": \"Chase Dodd\",\n",
        "    \"11\": \"Ryder Dodd\",\n",
        "    \"12\": \"Irving\"\n",
        "  },\n",
        "  \"Spain\": {\n",
        "    \"1\": \"Aguirre\",\n",
        "    \"2\": \"MunÃ¡rriz\",\n",
        "    \"3\": \"Granados\",\n",
        "    \"4\": \"Sanahuja\",\n",
        "    \"5\": \"De Toro\",\n",
        "    \"6\": \"Larumbe\",\n",
        "    \"7\": \"FamÄ›ra\",\n",
        "    \"8\": \"Cabanas\",\n",
        "    \"9\": \"Tahull\",\n",
        "    \"10\": \"Perrone\",\n",
        "    \"11\": \"Biel\",\n",
        "    \"12\": \"Bustos\",\n",
        "    \"13\": \"Lorrio\"\n",
        "  }\n",
        "}\n",
        "\n",
        "TEAM_COLORS = {\n",
        "  \"USA\": \"#0A3161\",     # USA navy\n",
        "  \"Spain\": \"#AA151B\"   # Spain red\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-kTisAQYQHM"
      },
      "source": [
        "## Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd1Yv077YSkj"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Video\n",
        "from typing import Dict, List, Optional, Union, Iterable, Tuple\n",
        "from operator import itemgetter\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "import supervision as sv\n",
        "from inference import get_model\n",
        "from sports import (\n",
        "    clean_paths,\n",
        "    ConsecutiveValueTracker,\n",
        "    TeamClassifier,\n",
        "    TeamClassifierByColor,\n",
        "    MeasurementUnit,\n",
        "    ViewTransformer,\n",
        ")\n",
        "\n",
        "from sports.waterpolo import (\n",
        "    PoolConfiguration,\n",
        "    League,\n",
        "    draw_pool,\n",
        "    draw_points_on_pool,\n",
        "    draw_paths_on_pool,\n",
        ")\n",
        "\n",
        "from sports.waterpolo.pool_keypoints import POOL_CORNER_KEYPOINT_INDICES\n",
        "\n",
        "# from sports import (\n",
        "#     clean_paths,\n",
        "#     ConsecutiveValueTracker,\n",
        "#     TeamClassifier,\n",
        "#     MeasurementUnit,\n",
        "#     ViewTransformer\n",
        "# )\n",
        "# from sports.basketball import (\n",
        "#     CourtConfiguration,\n",
        "#     League,\n",
        "#     draw_court,\n",
        "#     draw_points_on_court,\n",
        "#     draw_paths_on_court,\n",
        "#     draw_made_and_miss_on_court,\n",
        "#     ShotEventTracker\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PupZT1xHhmoG"
      },
      "source": [
        "## Object detection\n",
        "\n",
        "The model used in this notebook detects the following classes: `ball`, `number`, `player`, `player-in-possession`. The following classes, if added, could enable tracking of game events, player actions, and ball location for water polo analytics. `ball-in-basket`, `player-skip-shot`, `player-shot-block`, `referee`, and `goal`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5UNfe7fLvKf"
      },
      "source": [
        "### Load RF-DETR object detection model [ORIGINAL]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAD4DURmgz-s"
      },
      "outputs": [],
      "source": [
        "PLAYER_DETECTION_MODEL_ID = \"waterpolo-player-detection/2\"\n",
        "PLAYER_DETECTION_MODEL_CONFIDENCE = 0.4\n",
        "PLAYER_DETECTION_MODEL_IOU_THRESHOLD = 0.9\n",
        "PLAYER_DETECTION_MODEL = get_model(model_id=PLAYER_DETECTION_MODEL_ID)\n",
        "\n",
        "COLOR = sv.ColorPalette.from_hex([\n",
        "    \"#ffff00\", \"#ff9b00\", \"#ff66ff\", \"#3399ff\", \"#ff66b2\", \"#ff8080\",\n",
        "    \"#b266ff\", \"#9999ff\", \"#66ffff\", \"#33ff99\", \"#66ff66\", \"#99ff00\"\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXhPRYd8oJiz"
      },
      "source": [
        "Single Frame Object Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMrFhpMfoOBV"
      },
      "outputs": [],
      "source": [
        "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
        "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
        "detections = sv.Detections.from_inference(result)\n",
        "\n",
        "annotated_frame = frame.copy()\n",
        "annotated_frame = box_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=detections)\n",
        "annotated_frame = label_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=detections)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VzpR_LGEGad"
      },
      "source": [
        "### Keep only \"number\" class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vftat_boERYg"
      },
      "outputs": [],
      "source": [
        "NUMBER_CLASS_ID = 1 # ball=1, number=1, player=2, player-in-possession=3, referee=4\n",
        "\n",
        "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
        "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
        "detections = sv.Detections.from_inference(result)\n",
        "detections = detections[detections.class_id == NUMBER_CLASS_ID]\n",
        "\n",
        "annotated_frame = frame.copy()\n",
        "annotated_frame = box_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=detections)\n",
        "annotated_frame = label_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=detections)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttZNW8qgPLBm"
      },
      "source": [
        "### Keep only player-related classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjM1OuY5POCA"
      },
      "outputs": [],
      "source": [
        "PLAYER_CLASS_IDS = [2, 3] # player, player-in-possession\n",
        "\n",
        "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
        "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
        "detections = sv.Detections.from_inference(result)\n",
        "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
        "\n",
        "annotated_frame = frame.copy()\n",
        "annotated_frame = box_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=detections)\n",
        "annotated_frame = label_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=detections)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUZHIvfdNRi1"
      },
      "source": [
        "### Full video object detection\n",
        "\n",
        "We are running RF-DETR across all frames to produce a per-frame sequence of detections. These sequences seed tracking and provide number crops over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4O6W49jNz90"
      },
      "outputs": [],
      "source": [
        "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-detection{SOURCE_VIDEO_PATH.suffix}\"\n",
        "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-detection{TARGET_VIDEO_PATH.suffix}\"\n",
        "\n",
        "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
        "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
        "\n",
        "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
        "    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
        "    detections = sv.Detections.from_inference(result)\n",
        "\n",
        "    annotated_frame = frame.copy()\n",
        "    annotated_frame = box_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=detections)\n",
        "    annotated_frame = label_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=detections)\n",
        "    return annotated_frame\n",
        "\n",
        "sv.process_video(\n",
        "    source_path=SOURCE_VIDEO_PATH,\n",
        "    target_path=TARGET_VIDEO_PATH,\n",
        "    callback=callback,\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQ2PGjhHp8I7"
      },
      "outputs": [],
      "source": [
        "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzfEx2GbQNuO"
      },
      "source": [
        "## Player tracking\n",
        "\n",
        "We are switching from frame-wise boxes to temporal tracks. SAM2 yields per-player masks and stable track IDs that persist through occlusions and re-entries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6uqFLygS39e"
      },
      "source": [
        "### Load SAM2 tracking model\n",
        "\n",
        "We are loading a SAM2 checkpoint and config into the camera predictor. The large variant yields the highest quality masks; swap to smaller for speed if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2xcYSWjQVMu"
      },
      "outputs": [],
      "source": [
        "%cd $HOME/segment-anything-2-real-time\n",
        "\n",
        "from sam2.build_sam import build_sam2_camera_predictor\n",
        "\n",
        "SAM2_CHECKPOINT = \"checkpoints/sam2.1_hiera_small.pt\"\n",
        "SAM2_CONFIG = \"configs/sam2.1/sam2.1_hiera_s.yaml\"\n",
        "\n",
        "predictor = build_sam2_camera_predictor(SAM2_CONFIG, SAM2_CHECKPOINT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYfdZW-nTg5N"
      },
      "source": [
        "### Full video player tacking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csTvkOMoXZU4"
      },
      "source": [
        "We are prompting SAM2 with RF-DETR boxes and tracking across the clip. The callback saves masks, IDs, and visualizations for downstream use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz_lvB_tQOA4"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "class SAM2Tracker:\n",
        "    def __init__(self, predictor) -> None:\n",
        "        self.predictor = predictor\n",
        "        self._prompted = False\n",
        "\n",
        "    def prompt_first_frame(self, frame: np.ndarray, detections: sv.Detections) -> None:\n",
        "        if len(detections) == 0:\n",
        "            raise ValueError(\"detections must contain at least one box\")\n",
        "\n",
        "        if detections.tracker_id is None:\n",
        "            detections.tracker_id = list(range(1, len(detections) + 1))\n",
        "\n",
        "        with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.float16):\n",
        "          self.predictor.load_first_frame(frame)\n",
        "          for xyxy, obj_id in zip(detections.xyxy, detections.tracker_id):\n",
        "                  bbox = np.asarray([xyxy], dtype=np.float32)\n",
        "                  self.predictor.add_new_prompt(\n",
        "                      frame_idx=0,\n",
        "                      obj_id=int(obj_id),\n",
        "                      bbox=bbox,\n",
        "                  )\n",
        "\n",
        "        self._prompted = True\n",
        "\n",
        "    def propagate(self, frame: np.ndarray) -> sv.Detections:\n",
        "        if not self._prompted:\n",
        "            raise RuntimeError(\"Call prompt_first_frame before propagate\")\n",
        "\n",
        "        with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.float16):\n",
        "          tracker_ids, mask_logits = self.predictor.track(frame)\n",
        "\n",
        "        tracker_ids = np.asarray(tracker_ids, dtype=np.int32)\n",
        "        masks = (mask_logits > 0.0).cpu().numpy()\n",
        "        masks = np.squeeze(masks).astype(bool)\n",
        "\n",
        "        if masks.ndim == 2:\n",
        "            masks = masks[None, ...]\n",
        "\n",
        "        masks = np.array([\n",
        "            sv.filter_segments_by_distance(mask, relative_distance=0.03, mode=\"edge\")\n",
        "            for mask in masks\n",
        "        ])\n",
        "\n",
        "        xyxy = sv.mask_to_xyxy(masks=masks)\n",
        "        detections = sv.Detections(xyxy=xyxy, mask=masks, tracker_id=tracker_ids)\n",
        "        return detections\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        self._prompted = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcY0uJKOQnr-"
      },
      "outputs": [],
      "source": [
        "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-mask{SOURCE_VIDEO_PATH.suffix}\"\n",
        "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
        "\n",
        "# define team annotators\n",
        "\n",
        "mask_annotator = sv.MaskAnnotator(\n",
        "    color=COLOR,\n",
        "    color_lookup=sv.ColorLookup.TRACK,\n",
        "    opacity=0.5)\n",
        "box_annotator = sv.BoxAnnotator(\n",
        "    color=COLOR,\n",
        "    color_lookup=sv.ColorLookup.TRACK,\n",
        "    thickness=2\n",
        ")\n",
        "\n",
        "# we use RF-DETR model to aquire future SAM2 prompt\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
        "detections = sv.Detections.from_inference(result)\n",
        "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
        "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
        "\n",
        "annotated_frame = frame.copy()\n",
        "annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
        "sv.plot_image(annotated_frame)\n",
        "\n",
        "# we prompt SAM2 using RF-DETR model detections\n",
        "\n",
        "tracker = SAM2Tracker(predictor)\n",
        "tracker.prompt_first_frame(frame, detections)\n",
        "\n",
        "# we propagate tacks across all video frames\n",
        "\n",
        "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
        "    detections = tracker.propagate(frame)\n",
        "    annotated_frame = frame.copy()\n",
        "    annotated_frame = mask_annotator.annotate(scene=annotated_frame, detections=detections)\n",
        "    annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
        "    return annotated_frame\n",
        "\n",
        "sv.process_video(\n",
        "    source_path=SOURCE_VIDEO_PATH,\n",
        "    target_path=TARGET_VIDEO_PATH,\n",
        "    callback=callback,\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fmNMnpBb4PX"
      },
      "outputs": [],
      "source": [
        "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNTHNNatcC_I"
      },
      "source": [
        "## Cluster players into teams\n",
        "\n",
        "We are assigning each track to a team without labels. The pipeline uses SigLIP embeddings, UMAP to 3D, then K-means with k=2 for final team IDs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx-Z3m9QhPIO"
      },
      "source": [
        "### Collecting training set\n",
        "\n",
        "We are sampling frames at 1 FPS, detecting players, and extracting number crops. Number regions emphasize cap color and texture while reducing background artifacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouVXww3JlHSM"
      },
      "outputs": [],
      "source": [
        "STRIDE = 30\n",
        "\n",
        "crops = []\n",
        "\n",
        "for video_path in sv.list_files_with_extensions(SOURCE_VIDEO_DIRECTORY, extensions=[\"mp4\", \"avi\", \"mov\"]):\n",
        "    frame_generator = sv.get_video_frames_generator(source_path=video_path, stride=STRIDE)\n",
        "\n",
        "    for frame in tqdm(frame_generator):\n",
        "\n",
        "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD, class_agnostic_nms=True)[0]\n",
        "        detections = sv.Detections.from_inference(result)\n",
        "        detections = detections[np.isin(detections.class_id, NUMBER_CLASS_ID)]\n",
        "\n",
        "        boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.9)\n",
        "        for box in boxes:\n",
        "            crops.append(sv.crop_image(frame, box))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5WyZ4tmlwgQ"
      },
      "outputs": [],
      "source": [
        "sv.plot_images_grid(\n",
        "    images=crops[:100],\n",
        "    grid_size=(10, 10),\n",
        "    size=(10, 10)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDIWnI9NmKI0"
      },
      "source": [
        "### Train and test clustering model\n",
        "\n",
        "We are computing SigLIP embeddings for crops, reducing with UMAP, and fitting K-means. A quick validation confirms separation by uniform appearance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1GEHMzUmO4z"
      },
      "outputs": [],
      "source": [
        "team_classifier = TeamClassifierByColor()\n",
        "#team_classifier = TeamClassifier(device=\"cuda\")\n",
        "team_classifier.fit(crops)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8rYeP5HmkUj"
      },
      "outputs": [],
      "source": [
        "teams = team_classifier.predict(crops)\n",
        "\n",
        "team_0 = [crop for crop, team in zip(crops, teams) if team == 0]\n",
        "team_1 = [crop for crop, team in zip(crops, teams) if team == 1]\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=team_0[:50],\n",
        "    grid_size=(5, 10),\n",
        "    size=(10, 5)\n",
        ")\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=team_1[:50],\n",
        "    grid_size=(5, 10),\n",
        "    size=(10, 5)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CdLxBkXKOca"
      },
      "source": [
        "### Test clustering model on single video frame\n",
        "\n",
        "We are applying the trained clustering to one frameâ€™s player crops. The output assigns provisional team IDs to confirm the mapping before full-video use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSdtN_9fmw_M"
      },
      "outputs": [],
      "source": [
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD, class_agnostic_nms=True)[0]\n",
        "detections = sv.Detections.from_inference(result)\n",
        "detections = detections[np.isin(detections.class_id, NUMBER_CLASS_ID)]\n",
        "\n",
        "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.9)\n",
        "crops = [sv.crop_image(frame, box) for box in boxes]\n",
        "teams = np.array(team_classifier.predict(crops))\n",
        "\n",
        "team_0 = [crop for crop, team in zip(crops, teams) if team == 0]\n",
        "team_1 = [crop for crop, team in zip(crops, teams) if team == 1]\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=team_0[:10],\n",
        "    grid_size=(1, 10),\n",
        "    size=(10, 1)\n",
        ")\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=team_1[:10],\n",
        "    grid_size=(1, 10),\n",
        "    size=(10, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMVitdaknPqa"
      },
      "source": [
        "Since we do not control which IDs the clustering algorithm assigns to the teams, after training and testing we must select one of the dictionaries below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cALjgKE-nFwJ"
      },
      "outputs": [],
      "source": [
        "# TEAM_NAMES = {\n",
        "#     0: \"New York Knicks\",\n",
        "#     1: \"Boston Celtics\",\n",
        "# }\n",
        "\n",
        "TEAM_NAMES = {\n",
        "    0: \"USA\",\n",
        "    1: \"Spain\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBKKa8MKo5j7"
      },
      "source": [
        "### Full video team clustering\n",
        "\n",
        "We are assigning team IDs to tracks once, then reusing them across frames via track IDs. This keeps colors and labels consistent throughout the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bF0IaqNTccu"
      },
      "outputs": [],
      "source": [
        "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-teams{SOURCE_VIDEO_PATH.suffix}\"\n",
        "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "# define team annotators\n",
        "\n",
        "team_colors = sv.ColorPalette.from_hex([\n",
        "    TEAM_COLORS[TEAM_NAMES[0]],\n",
        "    TEAM_COLORS[TEAM_NAMES[1]]\n",
        "])\n",
        "\n",
        "team_mask_annotator = sv.MaskAnnotator(\n",
        "    color=team_colors,\n",
        "    opacity=0.5,\n",
        "    color_lookup=sv.ColorLookup.INDEX\n",
        ")\n",
        "\n",
        "team_box_annotator = sv.BoxAnnotator(\n",
        "    color=team_colors,\n",
        "    thickness=2,\n",
        "    color_lookup=sv.ColorLookup.INDEX\n",
        ")\n",
        "\n",
        "# one inference: get both player and cap detections\n",
        "\n",
        "result = PLAYER_DETECTION_MODEL.infer(\n",
        "    frame,\n",
        "    confidence=PLAYER_DETECTION_MODEL_CONFIDENCE,\n",
        "    iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD,\n",
        "    class_agnostic_nms=True,\n",
        ")[0]\n",
        "detections = sv.Detections.from_inference(result)\n",
        "\n",
        "# players: for SAM2 and callback (one detection per tracked person)\n",
        "\n",
        "player_detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
        "player_detections.tracker_id = np.arange(1, len(player_detections.class_id) + 1)\n",
        "\n",
        "# caps: for team classification only (crops from cap boxes)\n",
        "\n",
        "cap_detections = detections[np.isin(detections.class_id, NUMBER_CLASS_ID)]\n",
        "cap_crops = [sv.crop_image(frame, xyxy) for xyxy in cap_detections.xyxy]\n",
        "cap_teams = np.array(team_classifier.predict(cap_crops)) if len(cap_crops) > 0 else np.array([])\n",
        "\n",
        "# map each player to a team: assign the team of the cap whose center falls inside that player box\n",
        "\n",
        "TEAMS = np.zeros(len(player_detections), dtype=int)\n",
        "for cap_idx, (cap_xyxy, team) in enumerate(zip(cap_detections.xyxy, cap_teams)):\n",
        "    cap_cx = (cap_xyxy[0] + cap_xyxy[2]) / 2\n",
        "    cap_cy = (cap_xyxy[1] + cap_xyxy[3]) / 2\n",
        "    for player_idx, player_xyxy in enumerate(player_detections.xyxy):\n",
        "        if (\n",
        "            player_xyxy[0] <= cap_cx <= player_xyxy[2]\n",
        "            and player_xyxy[1] <= cap_cy <= player_xyxy[3]\n",
        "        ):\n",
        "            TEAMS[player_idx] = team\n",
        "            break\n",
        "\n",
        "# prompt SAM2 with player detections\n",
        "\n",
        "tracker = SAM2Tracker(predictor)\n",
        "tracker.prompt_first_frame(frame, player_detections)\n",
        "\n",
        "# propagate tracks and annotate by team\n",
        "\n",
        "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
        "    detections = tracker.propagate(frame)\n",
        "    annotated_frame = frame.copy()\n",
        "    annotated_frame = team_mask_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=detections,\n",
        "        custom_color_lookup=TEAMS[detections.tracker_id - 1]\n",
        "    )\n",
        "    annotated_frame = team_box_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=detections,\n",
        "        custom_color_lookup=TEAMS[detections.tracker_id - 1]\n",
        "    )\n",
        "    return annotated_frame\n",
        "\n",
        "sv.process_video(\n",
        "    source_path=SOURCE_VIDEO_PATH,\n",
        "    target_path=TARGET_VIDEO_PATH,\n",
        "    callback=callback,\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}\n",
        "\n",
        "# TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-teams{SOURCE_VIDEO_PATH.suffix}\"\n",
        "# TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
        "# frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "# frame = next(frame_generator)\n",
        "\n",
        "# # define team annotators\n",
        "\n",
        "# team_colors = sv.ColorPalette.from_hex([\n",
        "#     TEAM_COLORS[TEAM_NAMES[0]],\n",
        "#     TEAM_COLORS[TEAM_NAMES[1]]\n",
        "# ])\n",
        "\n",
        "# team_mask_annotator = sv.MaskAnnotator(\n",
        "#     color=team_colors,\n",
        "#     opacity=0.5,\n",
        "#     color_lookup=sv.ColorLookup.INDEX\n",
        "# )\n",
        "\n",
        "# team_box_annotator = sv.BoxAnnotator(\n",
        "#     color=team_colors,\n",
        "#     thickness=2,\n",
        "#     color_lookup=sv.ColorLookup.INDEX\n",
        "# )\n",
        "\n",
        "# # we use RF-DETR model to aquire future SAM2 prompt\n",
        "\n",
        "# result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
        "# detections = sv.Detections.from_inference(result)\n",
        "\n",
        "\n",
        "# detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
        "# detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
        "\n",
        "# # we determine the team for each player and assign a team ID to every detection\n",
        "\n",
        "# boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
        "# crops = [sv.crop_image(frame, box) for box in boxes]\n",
        "# TEAMS = np.array(team_classifier.predict(crops))\n",
        "\n",
        "# # we prompt SAM2 using RF-DETR model detections\n",
        "\n",
        "# tracker = SAM2Tracker(predictor)\n",
        "# tracker.prompt_first_frame(frame, detections)\n",
        "\n",
        "# # we propagate tacks across all video frames\n",
        "\n",
        "# def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
        "#     detections = tracker.propagate(frame)\n",
        "#     annotated_frame = frame.copy()\n",
        "#     annotated_frame = team_mask_annotator.annotate(\n",
        "#         scene=annotated_frame,\n",
        "#         detections=detections,\n",
        "#         custom_color_lookup=TEAMS[detections.tracker_id - 1]\n",
        "#     )\n",
        "#     annotated_frame = team_box_annotator.annotate(\n",
        "#         scene=annotated_frame,\n",
        "#         detections=detections,\n",
        "#         custom_color_lookup=TEAMS[detections.tracker_id - 1]\n",
        "#     )\n",
        "#     return annotated_frame\n",
        "\n",
        "# sv.process_video(\n",
        "#     source_path=SOURCE_VIDEO_PATH,\n",
        "#     target_path=TARGET_VIDEO_PATH,\n",
        "#     callback=callback,\n",
        "#     show_progress=True\n",
        "# )\n",
        "\n",
        "# !ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcYjgSQrVQP0"
      },
      "outputs": [],
      "source": [
        "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0Q-gOy2su5v"
      },
      "source": [
        "## Player numbers OCR\n",
        "\n",
        "We are moving to jersey OCR to identify individuals within each team. Number reads pair with tracks and teams to resolve names later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXiZXnqntShC"
      },
      "source": [
        "### Load number recognition model\n",
        "\n",
        "We are loading the fine-tuned SmolVLM2 OCR model by ID. It was trained on jersey crops and outputs digit strings suitable for downstream validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfgolXlwtep1"
      },
      "outputs": [],
      "source": [
        "NUMBER_RECOGNITION_MODEL_ID = \"water-polo-cap-numbers-ocr/2\"\n",
        "NUMBER_RECOGNITION_MODEL = get_model(model_id=NUMBER_RECOGNITION_MODEL_ID)\n",
        "NUMBER_RECOGNITION_MODEL_PROMPT = \"Read the number.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkM_JQ61s3Oc"
      },
      "source": [
        "### Single frame player number detection and recognition\n",
        "\n",
        "We are detecting number boxes, padding, and cropping. We then run SmolVLM2 on each crop and preview predictions next to the regions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ily4KtIdswTB"
      },
      "outputs": [],
      "source": [
        "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
        "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "frame = next(frame_generator)\n",
        "frame_h, frame_w, *_ = frame.shape\n",
        "\n",
        "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
        "detections = sv.Detections.from_inference(result)\n",
        "detections = detections[detections.class_id == NUMBER_CLASS_ID]\n",
        "\n",
        "annotated_frame = frame.copy()\n",
        "annotated_frame = box_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=detections)\n",
        "annotated_frame = label_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=detections)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXUi2CR-t4mt"
      },
      "outputs": [],
      "source": [
        "crops = [\n",
        "    sv.resize_image(sv.crop_image(frame, xyxy), resolution_wh=(224, 224))\n",
        "    for xyxy\n",
        "    in sv.clip_boxes(sv.pad_boxes(xyxy=detections.xyxy, px=10, py=10), (frame_w, frame_h))\n",
        "]\n",
        "numbers = [\n",
        "    NUMBER_RECOGNITION_MODEL.predict(crop, NUMBER_RECOGNITION_MODEL_PROMPT)[0]\n",
        "    for crop\n",
        "    in crops\n",
        "]\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=crops[:10],\n",
        "    titles=numbers[:10],\n",
        "    grid_size=(1, 10),\n",
        "    size=(10, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3fw2eXcuyVA"
      },
      "source": [
        "### Single frame player detection with number detection matching\n",
        "\n",
        "We are matching numbers to players using Intersection over Smaller Area. IoS equals 1.0 implies the number lies fully inside the player mask, so we link them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFB3xRaPzVyr"
      },
      "outputs": [],
      "source": [
        "def coords_above_threshold(\n",
        "    matrix: np.ndarray, threshold: float, sort_desc: bool = True\n",
        ") -> List[Tuple[int, int]]:\n",
        "    \"\"\"\n",
        "    Return all (row_index, col_index) where value > threshold.\n",
        "    Rows and columns may repeat.\n",
        "    Optionally sort by value descending.\n",
        "    \"\"\"\n",
        "    A = np.asarray(matrix)\n",
        "    rows, cols = np.where(A > threshold)\n",
        "    pairs = list(zip(rows.tolist(), cols.tolist()))\n",
        "    if sort_desc:\n",
        "        pairs.sort(key=lambda rc: A[rc[0], rc[1]], reverse=True)\n",
        "    return pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wJljIgOVq8D"
      },
      "outputs": [],
      "source": [
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "# define team annotators\n",
        "\n",
        "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=4, color_lookup=sv.ColorLookup.TRACK)\n",
        "\n",
        "player_mask_annotator = sv.MaskAnnotator(color=COLOR.by_idx(3), opacity=0.8, color_lookup=sv.ColorLookup.INDEX)\n",
        "number_mask_annotator = sv.MaskAnnotator(color=COLOR.by_idx(0), opacity=0.8, color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "# we use RF-DETR model to aquire future SAM2 prompt\n",
        "\n",
        "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
        "detections = sv.Detections.from_inference(result)\n",
        "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
        "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
        "\n",
        "# we prompt SAM2 using RF-DETR model detections\n",
        "\n",
        "tracker = SAM2Tracker(predictor)\n",
        "tracker.prompt_first_frame(frame, detections)\n",
        "\n",
        "# we propagate tacks across all video frames\n",
        "\n",
        "for index, frame in tqdm(enumerate(frame_generator)):\n",
        "\n",
        "    # we only process the first video frame\n",
        "\n",
        "    if index > 0:\n",
        "        break\n",
        "\n",
        "    frame_h, frame_w, *_ = frame.shape\n",
        "\n",
        "    player_detections = tracker.propagate(frame)\n",
        "\n",
        "    # we use RF-DETR model to detect numbers\n",
        "\n",
        "    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
        "    number_detections = sv.Detections.from_inference(result)\n",
        "    number_detections = number_detections[number_detections.class_id == NUMBER_CLASS_ID]\n",
        "    number_detections.mask = sv.xyxy_to_mask(boxes=number_detections.xyxy, resolution_wh=(frame_w, frame_h))\n",
        "\n",
        "    # we use mask IoS to match numbers with players\n",
        "\n",
        "    iou = sv.mask_iou_batch(\n",
        "        masks_true=player_detections.mask,\n",
        "        masks_detection=number_detections.mask,\n",
        "        overlap_metric=sv.OverlapMetric.IOS\n",
        "    )\n",
        "\n",
        "    pairs = coords_above_threshold(iou, 0.9)\n",
        "    player_idx, number_idx = zip(*pairs)\n",
        "\n",
        "    # we visualize all the masks\n",
        "\n",
        "    annotated_frame = frame.copy()\n",
        "    annotated_frame = player_mask_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=player_detections)\n",
        "    annotated_frame = number_mask_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=number_detections)\n",
        "    sv.plot_image(annotated_frame)\n",
        "\n",
        "    # we visualize only matched pairs\n",
        "\n",
        "    player_detections = player_detections[np.array(player_idx)]\n",
        "    number_detections = number_detections[np.array(number_idx)]\n",
        "    number_detections.tracker_id = player_detections.tracker_id\n",
        "\n",
        "    annotated_frame = frame.copy()\n",
        "    annotated_frame = box_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=player_detections)\n",
        "    annotated_frame = box_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=number_detections)\n",
        "    sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqaXSQBVB5kV"
      },
      "source": [
        "### Validating recognized numbers\n",
        "\n",
        "We are confirming numbers across time using a consecutive-agreement threshold. The validator locks a number to a track only after repeated consistent reads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-zjlroYWm-q"
      },
      "outputs": [],
      "source": [
        "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-validated-numbers{SOURCE_VIDEO_PATH.suffix}\"\n",
        "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
        "\n",
        "number_validator = ConsecutiveValueTracker(n_consecutive=3)\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "# define team annotators\n",
        "\n",
        "mask_annotator = sv.MaskAnnotator(\n",
        "    color=COLOR,\n",
        "    color_lookup=sv.ColorLookup.TRACK,\n",
        "    opacity=0.7)\n",
        "box_annotator = sv.BoxAnnotator(\n",
        "    color=COLOR,\n",
        "    color_lookup=sv.ColorLookup.TRACK,\n",
        "    thickness=2)\n",
        "label_annotator = sv.LabelAnnotator(\n",
        "    color=COLOR,\n",
        "    color_lookup=sv.ColorLookup.TRACK,\n",
        "    text_color=sv.Color.BLACK,\n",
        "    text_scale=0.8)\n",
        "\n",
        "# we use RF-DETR model to aquire future SAM2 prompt\n",
        "\n",
        "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
        "detections = sv.Detections.from_inference(result)\n",
        "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
        "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
        "\n",
        "# we prompt SAM2 using RF-DETR model detections\n",
        "\n",
        "tracker = SAM2Tracker(predictor)\n",
        "tracker.prompt_first_frame(frame, detections)\n",
        "\n",
        "# we propagate tacks across all video frames\n",
        "\n",
        "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
        "    player_detections = tracker.propagate(frame)\n",
        "\n",
        "    # we perform number recognition at specific frame intervals\n",
        "\n",
        "    if index % 5 == 0:\n",
        "        frame_h, frame_w, *_ = frame.shape\n",
        "\n",
        "        # we use RF-DETR model to detect numbers\n",
        "\n",
        "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
        "        number_detections = sv.Detections.from_inference(result)\n",
        "        number_detections = number_detections[number_detections.class_id == NUMBER_CLASS_ID]\n",
        "        number_detections.mask = sv.xyxy_to_mask(boxes=number_detections.xyxy, resolution_wh=(frame_w, frame_h))\n",
        "\n",
        "        # we crop out numbers detection and run recognition model\n",
        "\n",
        "        number_crops = [\n",
        "            sv.crop_image(frame, xyxy)\n",
        "            for xyxy\n",
        "            in sv.clip_boxes(sv.pad_boxes(xyxy=number_detections.xyxy, px=10, py=10), (frame_w, frame_h))\n",
        "        ]\n",
        "        numbers = [\n",
        "            NUMBER_RECOGNITION_MODEL.predict(number_crop, NUMBER_RECOGNITION_MODEL_PROMPT)[0]\n",
        "            for number_crop\n",
        "            in number_crops\n",
        "        ]\n",
        "\n",
        "        # we use mask IoS to match numbers with players\n",
        "\n",
        "        iou = sv.mask_iou_batch(\n",
        "            masks_true=player_detections.mask,\n",
        "            masks_detection=number_detections.mask,\n",
        "            overlap_metric=sv.OverlapMetric.IOS\n",
        "        )\n",
        "\n",
        "        pairs = coords_above_threshold(iou, 0.9)\n",
        "\n",
        "        if pairs:\n",
        "\n",
        "            player_idx, number_idx = zip(*pairs)\n",
        "            player_idx = [i + 1 for i in player_idx]\n",
        "            number_idx = list(number_idx)\n",
        "\n",
        "            # we update number_validator state\n",
        "\n",
        "            numbers = [numbers[int(i)] for i in number_idx]\n",
        "            number_validator.update(tracker_ids=player_idx, values=numbers)\n",
        "\n",
        "    # we visualize boxes and masks\n",
        "\n",
        "    annotated_frame = frame.copy()\n",
        "    annotated_frame = mask_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=player_detections)\n",
        "    annotated_frame = box_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=player_detections)\n",
        "\n",
        "    # we extract validated numbers\n",
        "\n",
        "    numbers = number_validator.get_validated(tracker_ids=player_detections.tracker_id)\n",
        "\n",
        "    # we visualize numbers\n",
        "\n",
        "    annotated_frame = label_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=player_detections,\n",
        "        labels=numbers)\n",
        "\n",
        "    return annotated_frame\n",
        "\n",
        "\n",
        "sv.process_video(\n",
        "    source_path=SOURCE_VIDEO_PATH,\n",
        "    target_path=TARGET_VIDEO_PATH,\n",
        "    callback=callback,\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyYzxDolX6dR"
      },
      "outputs": [],
      "source": [
        "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp9TzWCZOmr9"
      },
      "source": [
        "## Player recognition\n",
        "\n",
        "We are overlaying names, numbers, team colors, and masks for each tracked player. The final render shows stable identities aligned with roster data across the full clip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnWZ2ZKgZJv0"
      },
      "outputs": [],
      "source": [
        "!gdown https://drive.google.com/drive/folders/1RBjpI5Xleb58lujeusxH0W5zYMMA4ytO -O {HOME / \"fonts\"} --folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnoBYFhcYfpV"
      },
      "outputs": [],
      "source": [
        "frames_history = []\n",
        "detections_history = []\n",
        "\n",
        "number_validator = ConsecutiveValueTracker(n_consecutive=3)\n",
        "team_validator = ConsecutiveValueTracker(n_consecutive=1)\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "# we use RF-DETR model to aquire future SAM2 prompt\n",
        "\n",
        "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
        "detections = sv.Detections.from_inference(result)\n",
        "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
        "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
        "\n",
        "# we determine the team for each player and assign a team ID to every detection\n",
        "\n",
        "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
        "crops = [sv.crop_image(frame, box) for box in boxes]\n",
        "TEAMS = np.array(team_classifier.predict(crops))\n",
        "\n",
        "team_validator.update(tracker_ids=detections.tracker_id, values=TEAMS)\n",
        "\n",
        "# we prompt SAM2 using RF-DETR model detections\n",
        "\n",
        "tracker = SAM2Tracker(predictor)\n",
        "tracker.prompt_first_frame(frame, detections)\n",
        "\n",
        "# we propagate tacks across all video frames\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "\n",
        "for index, frame in tqdm(enumerate(frame_generator)):\n",
        "    player_detections = tracker.propagate(frame)\n",
        "    frames_history.append(frame)\n",
        "    detections_history.append(player_detections)\n",
        "\n",
        "    # we perform number recognition at specific frame intervals\n",
        "\n",
        "    if index % 5 == 0:\n",
        "        frame_h, frame_w, *_ = frame.shape\n",
        "\n",
        "        # we use RF-DETR model to detect numbers\n",
        "\n",
        "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
        "        number_detections = sv.Detections.from_inference(result)\n",
        "        number_detections = number_detections[number_detections.class_id == NUMBER_CLASS_ID]\n",
        "        number_detections.mask = sv.xyxy_to_mask(boxes=number_detections.xyxy, resolution_wh=(frame_w, frame_h))\n",
        "\n",
        "        # we crop out numbers detection and run recognition model\n",
        "\n",
        "        number_crops = [\n",
        "            sv.crop_image(frame, xyxy)\n",
        "            for xyxy\n",
        "            in sv.clip_boxes(sv.pad_boxes(xyxy=number_detections.xyxy, px=10, py=10), (frame_w, frame_h))\n",
        "        ]\n",
        "        numbers = [\n",
        "            NUMBER_RECOGNITION_MODEL.predict(number_crop, NUMBER_RECOGNITION_MODEL_PROMPT)[0]\n",
        "            for number_crop\n",
        "            in number_crops\n",
        "        ]\n",
        "\n",
        "        # we use mask IoS to match numbers with players\n",
        "\n",
        "        iou = sv.mask_iou_batch(\n",
        "            masks_true=player_detections.mask,\n",
        "            masks_detection=number_detections.mask,\n",
        "            overlap_metric=sv.OverlapMetric.IOS\n",
        "        )\n",
        "\n",
        "        pairs = coords_above_threshold(iou, 0.9)\n",
        "\n",
        "        if pairs:\n",
        "\n",
        "            player_idx, number_idx = zip(*pairs)\n",
        "            player_idx = [i + 1 for i in player_idx]\n",
        "            number_idx = list(number_idx)\n",
        "\n",
        "            # we update number_validator state\n",
        "\n",
        "            numbers = [numbers[int(i)] for i in number_idx]\n",
        "            number_validator.update(tracker_ids=player_idx, values=numbers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1scdRpWZUgd"
      },
      "outputs": [],
      "source": [
        "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-result{SOURCE_VIDEO_PATH.suffix}\"\n",
        "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
        "\n",
        "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
        "\n",
        "team_colors = sv.ColorPalette.from_hex([\n",
        "    TEAM_COLORS[TEAM_NAMES[0]],\n",
        "    TEAM_COLORS[TEAM_NAMES[1]]\n",
        "])\n",
        "\n",
        "team_mask_annotator = sv.MaskAnnotator(\n",
        "    color=team_colors,\n",
        "    opacity=0.5,\n",
        "    color_lookup=sv.ColorLookup.INDEX)\n",
        "team_label_annotator = sv.RichLabelAnnotator(\n",
        "    font_path=f\"{HOME}/fonts/Staatliches-Regular.ttf\",\n",
        "    font_size=40,\n",
        "    color=team_colors,\n",
        "    text_color=sv.Color.WHITE,\n",
        "    text_position=sv.Position.BOTTOM_CENTER,\n",
        "    text_offset=(0, 10),\n",
        "    color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
        "    for frame, detections in tqdm(zip(frames_history, detections_history)):\n",
        "        detections = detections[detections.area > 100]\n",
        "\n",
        "        teams = team_validator.get_validated(tracker_ids=detections.tracker_id)\n",
        "        teams = np.array(teams).astype(int)\n",
        "        numbers = number_validator.get_validated(tracker_ids=detections.tracker_id)\n",
        "        numbers = np.array(numbers)\n",
        "\n",
        "        labels = [\n",
        "            f\"#{number} {TEAM_ROSTERS[TEAM_NAMES[team]].get(number)}\"\n",
        "            for number, team\n",
        "            in zip(numbers, teams)\n",
        "        ]\n",
        "\n",
        "        annotated_frame = frame.copy()\n",
        "        annotated_frame = team_mask_annotator.annotate(\n",
        "            scene=annotated_frame,\n",
        "            detections=detections,\n",
        "            custom_color_lookup=teams)\n",
        "        annotated_frame = team_label_annotator.annotate(\n",
        "            scene=annotated_frame,\n",
        "            detections=detections,\n",
        "            labels=labels,\n",
        "            custom_color_lookup=teams)\n",
        "\n",
        "        sink.write_frame(annotated_frame)\n",
        "\n",
        "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ul99bONuZp-I"
      },
      "outputs": [],
      "source": [
        "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-YW3b5Qi-U6"
      },
      "source": [
        "## Pool keypoints detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsKMDgdqjmnI"
      },
      "source": [
        "### Load keypoint detection model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyHvSFpSjiDV"
      },
      "outputs": [],
      "source": [
        "KEYPOINT_DETECTION_MODEL_ID = \"water-polo-pool-detection/1\"\n",
        "KEYPOINT_DETECTION_MODEL_CONFIDENCE = 0.3\n",
        "KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE = 0.5\n",
        "KEYPOINT_DETECTION_MODEL = get_model(model_id=KEYPOINT_DETECTION_MODEL_ID)\n",
        "KEYPOINT_COLOR = sv.Color.from_hex('#FF1493')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqchilTtkWz6"
      },
      "source": [
        "### Single frame keypoint detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJRCgmIBkRXx"
      },
      "outputs": [],
      "source": [
        "vertex_annotator = sv.VertexAnnotator(color=KEYPOINT_COLOR, radius=8)\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
        "key_points = sv.KeyPoints.from_inference(result)\n",
        "\n",
        "annotated_frame = frame.copy()\n",
        "annotated_frame = vertex_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    key_points=key_points)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCcYa07Dk-Lf"
      },
      "source": [
        "### Detecting keypoints with high confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4nHvCAGlDvF"
      },
      "outputs": [],
      "source": [
        "vertex_annotator = sv.VertexAnnotator(color=KEYPOINT_COLOR, radius=8)\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
        "key_points = sv.KeyPoints.from_inference(result)\n",
        "key_points = key_points[:, key_points.confidence[0] > KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE]\n",
        "\n",
        "annotated_frame = frame.copy()\n",
        "annotated_frame = vertex_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    key_points=key_points)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B_KM5MNlYBm"
      },
      "source": [
        "## Map player positions to court coordinates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdbDdYc10Pb-"
      },
      "source": [
        "### Single frame player position mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2r-NClenOfQ"
      },
      "outputs": [],
      "source": [
        "# Configure pool instead of court\n",
        "config = PoolConfiguration(league=League.MEN, measurement_unit=MeasurementUnit.FEET)\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "# define team annotators\n",
        "team_colors = sv.ColorPalette.from_hex([\n",
        "    TEAM_COLORS[TEAM_NAMES[0]],\n",
        "    TEAM_COLORS[TEAM_NAMES[1]]\n",
        "])\n",
        "\n",
        "team_box_annotator = sv.BoxAnnotator(\n",
        "    color=team_colors,\n",
        "    thickness=2,\n",
        "    color_lookup=sv.ColorLookup.INDEX\n",
        ")\n",
        "\n",
        "# we use RF-DETR model to acquire future SAM2 prompt\n",
        "result = PLAYER_DETECTION_MODEL.infer(\n",
        "    frame,\n",
        "    confidence=PLAYER_DETECTION_MODEL_CONFIDENCE,\n",
        "    iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD\n",
        ")[0]\n",
        "detections = sv.Detections.from_inference(result)\n",
        "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
        "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
        "\n",
        "# determine team for each player and assign a team ID\n",
        "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
        "crops = [sv.crop_image(frame, box) for box in boxes]\n",
        "TEAMS = np.array(team_classifier.predict(crops))\n",
        "\n",
        "annotated_frame = frame.copy()\n",
        "annotated_frame = team_box_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=detections,\n",
        "    custom_color_lookup=TEAMS[detections.tracker_id - 1]\n",
        ")\n",
        "sv.plot_image(annotated_frame)\n",
        "\n",
        "# we use a keypoint model to detect pool landmarks\n",
        "result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
        "key_points = sv.KeyPoints.from_inference(result)\n",
        "landmarks_mask = key_points.confidence[0] > KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE\n",
        "\n",
        "if np.count_nonzero(landmarks_mask) >= 4:\n",
        "\n",
        "    POOL_CORNER_KEYPOINT_INDICES = np.array([5, 0, 18, 13])\n",
        "\n",
        "corner_mask = landmarks_mask[POOL_CORNER_KEYPOINT_INDICES]\n",
        "\n",
        "if np.count_nonzero(corner_mask) >= 4:\n",
        "    pool_landmarks = np.array(config.vertices)[corner_mask]\n",
        "    frame_landmarks = key_points[:, POOL_CORNER_KEYPOINT_INDICES].xy[0][corner_mask]\n",
        "\n",
        "    frame_to_pool_transformer = ViewTransformer(\n",
        "        source=frame_landmarks,\n",
        "        target=pool_landmarks,\n",
        "    )\n",
        "\n",
        "    # # calculate homography matrix (frame -> pool)\n",
        "    # pool_landmarks = np.array(config.vertices)[landmarks_mask]\n",
        "    # frame_landmarks = key_points[:, landmarks_mask].xy[0]\n",
        "\n",
        "    # frame_to_pool_transformer = ViewTransformer(\n",
        "    #     source=frame_landmarks,\n",
        "    #     target=pool_landmarks,\n",
        "    # )\n",
        "\n",
        "    frame_xy = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
        "\n",
        "    if len(frame_xy) > 0:\n",
        "\n",
        "        # transform points into pool coordinates\n",
        "        pool_xy = frame_to_pool_transformer.transform_points(points=frame_xy)\n",
        "\n",
        "        # visualize on pool\n",
        "        pool = draw_pool(config=config)\n",
        "        pool = draw_points_on_pool(\n",
        "            config=config,\n",
        "            xy=pool_xy[TEAMS == 0],\n",
        "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[0]]),\n",
        "            pool=pool\n",
        "        )\n",
        "        pool = draw_points_on_pool(\n",
        "            config=config,\n",
        "            xy=pool_xy[TEAMS == 1],\n",
        "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[1]]),\n",
        "            pool=pool\n",
        "        )\n",
        "\n",
        "        sv.plot_image(pool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJiEzaE85jmW"
      },
      "source": [
        "### Full video player position mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzEhOEjQ3epJ"
      },
      "outputs": [],
      "source": [
        "video_xy = []\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "# we use RF-DETR model to acquire future SAM2 prompt\n",
        "result = PLAYER_DETECTION_MODEL.infer(\n",
        "    frame,\n",
        "    confidence=PLAYER_DETECTION_MODEL_CONFIDENCE,\n",
        "    iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD\n",
        ")[0]\n",
        "detections = sv.Detections.from_inference(result)\n",
        "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
        "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
        "\n",
        "# determine team for each player and assign a team ID to every detection\n",
        "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
        "crops = [sv.crop_image(frame, box) for box in boxes]\n",
        "TEAMS = np.array(team_classifier.predict(crops))\n",
        "\n",
        "# prompt SAM2 using RF-DETR model detections\n",
        "tracker = SAM2Tracker(predictor)\n",
        "tracker.prompt_first_frame(frame, detections)\n",
        "\n",
        "# propagate tracks across all video frames\n",
        "for frame_idx, frame in tqdm(enumerate(frame_generator)):\n",
        "    detections = tracker.propagate(frame)\n",
        "\n",
        "    # detect pool landmarks with keypoint model\n",
        "    result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
        "    key_points = sv.KeyPoints.from_inference(result)\n",
        "    landmarks_mask = key_points.confidence[0] > KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE\n",
        "\n",
        "    if np.count_nonzero(landmarks_mask) >= 4:\n",
        "        # calculate homography matrix (frame -> pool)\n",
        "        pool_landmarks = np.array(config.vertices)[landmarks_mask]\n",
        "        frame_landmarks = key_points[:, landmarks_mask].xy[0]\n",
        "\n",
        "        frame_to_pool_transformer = ViewTransformer(\n",
        "            source=frame_landmarks,\n",
        "            target=pool_landmarks,\n",
        "        )\n",
        "\n",
        "        frame_xy = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
        "\n",
        "        # transform video frame coordinates into pool coordinates\n",
        "        pool_xy = frame_to_pool_transformer.transform_points(points=frame_xy)\n",
        "        video_xy.append(pool_xy)\n",
        "\n",
        "# if lengths vary across frames, keep as object array\n",
        "video_xy = np.array(video_xy, dtype=object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAeep8b23zXs"
      },
      "outputs": [],
      "source": [
        "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-map{SOURCE_VIDEO_PATH.suffix}\"\n",
        "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
        "\n",
        "config = PoolConfiguration(league=League.MEN, measurement_unit=MeasurementUnit.FEET)\n",
        "pool = draw_pool(config=config)\n",
        "pool_h, pool_w, _ = pool.shape\n",
        "\n",
        "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
        "video_info.width = pool_w\n",
        "video_info.height = pool_h\n",
        "\n",
        "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
        "    for pool_xy in tqdm(video_xy):\n",
        "        pool = draw_pool(config=config)\n",
        "        pool = draw_points_on_pool(\n",
        "            config=config,\n",
        "            xy=pool_xy[TEAMS == 0],\n",
        "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[0]]),\n",
        "            pool=pool\n",
        "        )\n",
        "        pool = draw_points_on_pool(\n",
        "            config=config,\n",
        "            xy=pool_xy[TEAMS == 1],\n",
        "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[1]]),\n",
        "            pool=pool\n",
        "        )\n",
        "        sink.write_frame(pool)\n",
        "\n",
        "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeSmIUzW4SHN"
      },
      "outputs": [],
      "source": [
        "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUhD1VKNM41D"
      },
      "source": [
        "### Clean player movement paths\n",
        "\n",
        "We are detecting sudden jumps in position using robust speed analysis. We are removing short abnormal runs and nearby frames to eliminate teleport-like artifacts. We are filling missing segments with linear interpolation to ensure continuous motion. We are smoothing all paths with a Savitzkyâ€“Golay filter to achieve stable and natural movement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gYSJSEm7KG-"
      },
      "outputs": [],
      "source": [
        "pool = draw_paths_on_pool(\n",
        "    config=config,\n",
        "    paths=[video_xy[:, 0, :]],\n",
        ")\n",
        "sv.plot_image(pool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7T6ImF7A-9JF"
      },
      "outputs": [],
      "source": [
        "cleaned_xy, edited_mask = clean_paths(\n",
        "    video_xy,\n",
        "    jump_sigma=3.5,\n",
        "    min_jump_dist=0.6,\n",
        "    max_jump_run=18,\n",
        "    pad_around_runs=2,\n",
        "    smooth_window=9,\n",
        "    smooth_poly=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWjBXVMZNxAa"
      },
      "outputs": [],
      "source": [
        "def split_true_runs(mask: np.ndarray, coords) -> list[np.ndarray]:\n",
        "    mask = np.asarray(mask).squeeze()\n",
        "    idx = np.flatnonzero(mask)\n",
        "    if idx.size == 0:\n",
        "        return []\n",
        "\n",
        "    splits = np.where(np.diff(idx) > 1)[0] + 1\n",
        "    groups = np.split(idx, splits)\n",
        "\n",
        "    paths: list[np.ndarray] = []\n",
        "    if isinstance(coords, np.ndarray) and coords.dtype != object:\n",
        "        # Regular array: [frames, players, 2]\n",
        "        for g in groups:\n",
        "            paths.append(coords[g, 0, :])\n",
        "    else:\n",
        "        # Ragged/object array: coords[frame] -> np.ndarray shape [players, 2]\n",
        "        for g in groups:\n",
        "            segment = []\n",
        "            for i in g:\n",
        "                fi = coords[i]\n",
        "                if fi is not None and len(fi) > 0:\n",
        "                    segment.append(fi[0])\n",
        "            if segment:\n",
        "                paths.append(np.asarray(segment, dtype=float))\n",
        "    return paths\n",
        "\n",
        "\n",
        "# Build the full path for player 0 across frames (green)\n",
        "if isinstance(video_xy, np.ndarray) and video_xy.dtype != object:\n",
        "    player0_full_path = video_xy[:, 0, :]\n",
        "else:\n",
        "    player0_full_path = np.asarray(\n",
        "        [f[0] for f in video_xy if f is not None and len(f) > 0],\n",
        "        dtype=float\n",
        "    )\n",
        "\n",
        "pool = draw_paths_on_pool(\n",
        "    config=config,\n",
        "    paths=[player0_full_path],\n",
        "    color=sv.Color.GREEN,\n",
        ")\n",
        "\n",
        "# Overlay edited/cleaned true runs in red\n",
        "pool = draw_paths_on_pool(\n",
        "    config=config,\n",
        "    paths=split_true_runs(edited_mask[:, 0], video_xy),\n",
        "    color=sv.Color.RED,\n",
        "    pool=pool\n",
        ")\n",
        "\n",
        "sv.plot_image(pool)\n",
        "\n",
        "# def split_true_runs(mask: np.ndarray, coords: np.ndarray) -> list[np.ndarray]:\n",
        "#     mask = mask.squeeze()\n",
        "#     idx = np.flatnonzero(mask)\n",
        "#     if idx.size == 0:\n",
        "#         return []\n",
        "#     splits = np.where(np.diff(idx) > 1)[0] + 1\n",
        "#     groups = np.split(idx, splits)\n",
        "#     return [coords[g, 0, :] for g in groups]\n",
        "\n",
        "\n",
        "# court = draw_paths_on_court(\n",
        "#     config=config,\n",
        "#     paths=[video_xy[:, 0, :]],\n",
        "#     color=sv.Color.GREEN,\n",
        "# )\n",
        "\n",
        "# court = draw_paths_on_court(\n",
        "#     config=config,\n",
        "#     paths=split_true_runs(edited_mask[:, 0], video_xy),\n",
        "#     color=sv.Color.RED,\n",
        "#     court=court\n",
        "# )\n",
        "\n",
        "# sv.plot_image(court)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waPGh4My_Bs6"
      },
      "outputs": [],
      "source": [
        "pool_vis = draw_paths_on_pool(\n",
        "    config=config,\n",
        "    paths=[cleaned_xy[:, 0, :]],\n",
        ")\n",
        "sv.plot_image(pool_vis)\n",
        "\n",
        "# test = draw_paths_on_court(\n",
        "#     config=config,\n",
        "#     paths=[cleaned_xy[:, 0, :]],\n",
        "# )\n",
        "\n",
        "# sv.plot_image(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxCoYbu5_HSG"
      },
      "outputs": [],
      "source": [
        "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-map{SOURCE_VIDEO_PATH.suffix}\"\n",
        "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
        "\n",
        "config = PoolConfiguration(league=League.MEN, measurement_unit=MeasurementUnit.FEET)\n",
        "pool = draw_pool(config=config)\n",
        "pool_h, pool_w, _ = pool.shape\n",
        "\n",
        "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
        "video_info.width = pool_w\n",
        "video_info.height = pool_h\n",
        "\n",
        "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
        "    for frame_xy in tqdm(cleaned_xy):\n",
        "        pool = draw_pool(config=config)\n",
        "        pool = draw_points_on_pool(\n",
        "            config=config,\n",
        "            xy=frame_xy[TEAMS == 0],\n",
        "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[0]]),\n",
        "            pool=pool\n",
        "        )\n",
        "        pool = draw_points_on_pool(\n",
        "            config=config,\n",
        "            xy=frame_xy[TEAMS == 1],\n",
        "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[1]]),\n",
        "            pool=pool\n",
        "        )\n",
        "        sink.write_frame(pool)\n",
        "\n",
        "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_jZI2P8_SSo"
      },
      "outputs": [],
      "source": [
        "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6bu0UouKA1x"
      },
      "source": [
        "## Classify shorts as made or miss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hie0heVdMY06"
      },
      "source": [
        "### Keep only \"player-jump-shot\" class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3CU5QQ2KYtO"
      },
      "outputs": [],
      "source": [
        "SOURCE_VIDEO_PATH = SOURCE_VIDEO_DIRECTORY / \"boston-celtics-new-york-knicks-game-1-q1-03.16-03.11.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV9G1ZnNLf4v"
      },
      "outputs": [],
      "source": [
        "PLAYER_JUMP_SHOT_CLASS_ID = 5\n",
        "FRAME_IDX = 65\n",
        "\n",
        "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
        "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, start=FRAME_IDX, iterative_seek=True)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
        "detections = sv.Detections.from_inference(result)\n",
        "detections = detections[detections.class_id == PLAYER_JUMP_SHOT_CLASS_ID]\n",
        "\n",
        "annotated_frame = frame.copy()\n",
        "annotated_frame = box_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=detections)\n",
        "annotated_frame = label_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=detections)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNsjk7_PN1fW"
      },
      "source": [
        "### Mark jump-shot location on the court"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDS4Ug9gN4or"
      },
      "outputs": [],
      "source": [
        "config = CourtConfiguration(league=League.NBA, measurement_unit=MeasurementUnit.FEET)\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, start=FRAME_IDX, iterative_seek=True)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "# we use a RF-DETR model to detect players\n",
        "\n",
        "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
        "detections = sv.Detections.from_inference(result)\n",
        "detections = detections[detections.class_id == PLAYER_JUMP_SHOT_CLASS_ID]\n",
        "\n",
        "# we use a keypoint model to detect court landmarks\n",
        "\n",
        "result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
        "key_points = sv.KeyPoints.from_inference(result)\n",
        "landmarks_mask = key_points.confidence[0] > KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE\n",
        "\n",
        "if np.count_nonzero(landmarks_mask) >= 4:\n",
        "\n",
        "    # we calculate homography matrix\n",
        "\n",
        "    court_landmarks = np.array(config.vertices)[landmarks_mask]\n",
        "    frame_landmarks = key_points[:, landmarks_mask].xy[0]\n",
        "\n",
        "    frame_to_court_transformer = ViewTransformer(\n",
        "        source=frame_landmarks,\n",
        "        target=court_landmarks,\n",
        "    )\n",
        "\n",
        "    frame_xy = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
        "\n",
        "    # transform video frame coordinates into court coordinates\n",
        "\n",
        "    court_xy = frame_to_court_transformer.transform_points(points=frame_xy)\n",
        "\n",
        "    court = draw_made_and_miss_on_court(\n",
        "        config=config,\n",
        "        made_xy=court_xy,\n",
        "        made_size=25,\n",
        "        made_color=sv.Color.from_hex(\"#007A33\"),\n",
        "        made_thickness=6,\n",
        "        line_thickness=4\n",
        "    )\n",
        "\n",
        "    sv.plot_image(court)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R6foiAhRaAd"
      },
      "source": [
        "## Detect shot events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kH3iKTmRhx_"
      },
      "outputs": [],
      "source": [
        "BALL_IN_BASKET_CLASS_ID = 1\n",
        "JUMP_SHOT_CLASS_ID = 5\n",
        "LAYUP_DUNK_CLASS_ID = 6\n",
        "\n",
        "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
        "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
        "\n",
        "shot_event_tracker = ShotEventTracker(\n",
        "    reset_time_frames=int(video_info.fps * 1.7),\n",
        "    minimum_frames_between_starts=int(video_info.fps * 0.5),\n",
        "    cooldown_frames_after_made=int(video_info.fps * 0.5),\n",
        ")\n",
        "\n",
        "for frame_index, frame in enumerate(frame_generator):\n",
        "\n",
        "    # we use a RF-DETR model to detect jump shot, layup, dunk and ball in basket\n",
        "\n",
        "    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
        "    detections = sv.Detections.from_inference(result)\n",
        "\n",
        "    has_jump_shot = len(detections[detections.class_id == JUMP_SHOT_CLASS_ID]) > 0\n",
        "    has_layup_dunk = len(detections[detections.class_id == LAYUP_DUNK_CLASS_ID]) > 0\n",
        "    has_ball_in_basket = len(detections[detections.class_id == BALL_IN_BASKET_CLASS_ID]) > 0\n",
        "\n",
        "    events = shot_event_tracker.update(\n",
        "        frame_index=frame_index,\n",
        "        has_jump_shot=has_jump_shot,\n",
        "        has_layup_dunk=has_layup_dunk,\n",
        "        has_ball_in_basket=has_ball_in_basket,\n",
        "    )\n",
        "\n",
        "    if events:\n",
        "        print(events)\n",
        "\n",
        "        annotated_frame = frame.copy()\n",
        "        annotated_frame = box_annotator.annotate(\n",
        "            scene=annotated_frame,\n",
        "            detections=detections)\n",
        "        annotated_frame = label_annotator.annotate(\n",
        "            scene=annotated_frame,\n",
        "            detections=detections)\n",
        "\n",
        "        sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARvSRBTLaZXD"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <p>\n",
        "    Looking for more tutorials or have questions?\n",
        "    Check out our <a href=\"https://github.com/roboflow/notebooks\">GitHub repo</a> for more notebooks,\n",
        "    or visit our <a href=\"https://discord.gg/GbfgXGJ8Bk\">discord</a>.\n",
        "  </p>\n",
        "  \n",
        "  <p>\n",
        "    <strong>If you found this helpful, please consider giving us a â­\n",
        "    <a href=\"https://github.com/roboflow/notebooks\">on GitHub</a>!</strong>\n",
        "  </p>\n",
        "\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}